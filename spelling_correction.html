Reuters-21578 text categorization test collection Distribution 1.0 README file (v 1.2) <u><b>26</b></u> September <u><b>1997</b></u> David D. Lewis AT&amp;T Labs <u><b>-</b></u> Research lewis@research.att.com I. Introduction This README <u><b>describes</b></u> Distribution 1.0 of the Reuters-21578 text categorization test collection, a resource for research in information retrieval, machine learning, and other <u><b>corpus-based</b></u> research. II. Copyright &amp; Notification The copyright for the text of <u><b>newswire</b></u> <u><b>articles</b></u> and Reuters <u><b>annotations</b></u> in the Reuters-21578 collection <u><b>resides</b></u> with Reuters Ltd. Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free distribution of this data *for research <u><b>purposes</b></u> only*. If you publish <u><b>results</b></u> based on this data set, please acknowledge its use, refer to the data set by the name &quot;Reuters-21578, Distribution 1.0&quot;, and inform your <u><b>readers</b></u> of the current location of the data set (see &quot;Availability &amp; Questions&quot;). III. Availability &amp; Questions The Reuters-21578, Distribution 1.0 test collection is available from David D. Lewis' professional home page, <u><b>currently:</b></u> http://www.research.att.com/~lewis Besides this README file, the collection <u><b>consists</b></u> of <u><b>22</b></u> data files, an SGML DTD file <u><b>describing</b></u> the data file format, and six <u><b>files</b></u> <u><b>describing</b></u> the <u><b>categories</b></u> used to index the data. (See Sections VI and VII for more details.) Some additional files, which are not part of the collection but have been <u><b>contributed</b></u> by other <u><b>researchers</b></u> as useful <u><b>resources</b></u> are also included. All <u><b>files</b></u> are available uncompressed, and in addition a single <u><b>gzipped</b></u> Unix tar archive of the entire distribution is available as reuters21578.tar.gz. The text categorization <u><b>mailing</b></u> list, DDLBETA, is a good place to send <u><b>questions</b></u> about this collection and other text categorization issues. You may join the list by writing David Lewis at lewis@research.att.com. IV. History &amp; Acknowledgements The <u><b>documents</b></u> in the Reuters-21578 collection <u><b>appeared</b></u> on the Reuters <u><b>newswire</b></u> in 1987. The <u><b>documents</b></u> were <u><b>assembled</b></u> and indexed with <u><b>categories</b></u> by personnel from Reuters Ltd. (Sam Dobbins, Mike Topliss, Steve Weinstein) and Carnegie Group, Inc. (Peggy Andersen, Monica Cellio, Phil Hayes, Laura Knecht, Irene Nirenburg) in 1987. In 1990, the <u><b>documents</b></u> were made available by Reuters and CGI for research <u><b>purposes</b></u> to the Information Retrieval Laboratory (W. Bruce Croft, Director) of the Computer and Information Science Department at the University of Massachusetts at Amherst. Formatting of the <u><b>documents</b></u> and production of associated data <u><b>files</b></u> was done in <u><b>1990</b></u> by David D. Lewis and Stephen Harding at the Information Retrieval Laboratory. Further <u><b>formatting</b></u> and data file production was done in <u><b>1991</b></u> and <u><b>1992</b></u> by David D. Lewis and Peter Shoemaker at the Center for Information and Language Studies, University of Chicago. This version of the data was made available for anonymous FTP as &quot;Reuters-22173, Distribution 1.0&quot; in January 1993. From <u><b>1993</b></u> through 1996, Distribution 1.0 was <u><b>hosted</b></u> at a succession of FTP <u><b>sites</b></u> <u><b>maintained</b></u> by the Center for Intelligent Information Retrieval (W. Bruce Croft, Director) of the Computer Science Department at the University of Massachusetts at Amherst. At the ACM SIGIR <u><b>'96</b></u> conference in August, <u><b>1996</b></u> a group of text categorization <u><b>researchers</b></u> <u><b>discussed</b></u> how <u><b>published</b></u> <u><b>results</b></u> on Reuters-22173 could be made more comparable across studies. It was decided that a new version of collection should be produced with less ambiguous formatting, and <u><b>including</b></u> documentation carefully spelling out standard <u><b>methods</b></u> of <u><b>using</b></u> the collection. The opportunity would also be used to correct a variety of typographical and other <u><b>errors</b></u> in the categorization and <u><b>formatting</b></u> of the collection. Steve Finch and David D. Lewis did this cleanup of the collection September through November of 1996, <u><b>relying</b></u> heavily on Finch's SGML-tagged version of the collection from an <u><b>earlier</b></u> study. One result of the <u><b>re-examination</b></u> of the collection was the removal of <u><b>595</b></u> <u><b>documents</b></u> which were exact <u><b>duplicates</b></u> (based on identity of <u><b>timestamps</b></u> down to the second) of other <u><b>documents</b></u> in the collection. The new collection therefore <u><b>has</b></u> only 21,578 documents, and thus is <u><b>called</b></u> the Reuters-21578 collection. This README <u><b>describes</b></u> version 1.0 of this new collection, which we refer to as &quot;Reuters-21578, Distribution 1.0&quot;. In <u><b>preparing</b></u> the collection and documentation we have <u><b>benefited</b></u> from <u><b>discussions</b></u> with Eric Brown, William Cohen, Fred Damerau, Yoram Singer, Amit Singhal, and Yiming Yang, among many others. We thank all the people and <u><b>organizations</b></u> listed above for their <u><b>efforts</b></u> and support, without which this collection would not exist. A variety of other <u><b>changes</b></u> were also made in going from Reuters-22173 to Reuters-21578: 1. Documents were marked up with SGML tags, and a corresponding SGML DTD was produced, so that the <u><b>boundaries</b></u> of important <u><b>sections</b></u> of <u><b>documents</b></u> (e.g. category fields) are unambiguous. 2. The set of <u><b>categories</b></u> that are legal for each of the five <u><b>controlled</b></u> vocabulary <u><b>fields</b></u> was specified. All category <u><b>names</b></u> not legal for a field were corrected to a legal category, <u><b>moved</b></u> to their appropriate field, or removed, as appropriate. 3. Documents were given new ID numbers, in chronological order, and are collected <u><b>1000</b></u> to a file in order by ID (and therefore in order chronologically). V. What is a Text Categorization Test Collection and Who Cares? *Text categorization* is the task of <u><b>deciding</b></u> whether a piece of text <u><b>belongs</b></u> to any of a set of <u><b>prespecified</b></u> categories. It is a generic text <u><b>processing</b></u> task useful in indexing <u><b>documents</b></u> for later retrieval, as a stage in natural language <u><b>processing</b></u> systems, for content analysis, and in many other <u><b>roles</b></u> [LEWIS94d]. The use of standard, widely distributed test <u><b>collections</b></u> <u><b>has</b></u> been a considerable aid in the development of <u><b>algorithms</b></u> for the related task of *text retrieval* (finding <u><b>documents</b></u> that satisfy a particular <u><b>user's</b></u> information need, usually expressed in an textual request). Text retrieval test <u><b>collections</b></u> have <u><b>allowed</b></u> the comparison of <u><b>algorithms</b></u> <u><b>developed</b></u> by a variety of <u><b>researchers</b></u> around the world. (For more on text retrieval test <u><b>collections</b></u> see SPARCKJONES76.) Standard test <u><b>collections</b></u> have been lacking, however, for text categorization. Few data <u><b>sets</b></u> have been used by more than one researcher, making <u><b>results</b></u> hard to compare. The Reuters-22173 test collection <u><b>has</b></u> been used in a number of <u><b>published</b></u> <u><b>studies</b></u> since it was made available, and we believe that the Reuters-21578 collection will be even more valuable. The collection may also be of interest to <u><b>researchers</b></u> in machine learning, as it <u><b>provides</b></u> a classification task with <u><b>challenging</b></u> properties. There are multiple categories, the <u><b>categories</b></u> are <u><b>overlapping</b></u> and nonexhaustive, and there are <u><b>relationships</b></u> among the categories. There are interesting <u><b>possibilities</b></u> for the use of domain knowledge. There are many possible feature <u><b>sets</b></u> that can be extracted from the text, and most plausible <u><b>feature/example</b></u> matrices are large and sparse. There is even some temporal structure to the data [LEWIS94b], though <u><b>problems</b></u> with the indexing and the uneven distribution of <u><b>stories</b></u> within the <u><b>timespan</b></u> covered may make this collection a poor one to explore temporal issues. VI. Formatting The Reuters-21578 collection is distributed in <u><b>22</b></u> files. Each of the first <u><b>21</b></u> <u><b>files</b></u> (reut2-000.sgm through reut2-020.sgm) contain <u><b>1000</b></u> documents, while the last (reut2-021.sgm) <u><b>contains</b></u> <u><b>578</b></u> documents. The <u><b>files</b></u> are in SGML format. Rather than going into the <u><b>details</b></u> of the SGML language, we describe here in an informal way how the SGML <u><b>tags</b></u> are used to divide each file, and each document, into sections. Readers interested in more detail on SGML are <u><b>encouraged</b></u> to pursue one of the many <u><b>books</b></u> and web <u><b>pages</b></u> on the subject. Each of the <u><b>22</b></u> <u><b>files</b></u> <u><b>begins</b></u> with a document type declaration <u><b>line:</b></u> &lt;!DOCTYPE lewis SYSTEM &quot;lewis.dtd&quot;&gt; The DTD file lewis.dtd is included in the distribution. Following the document type declaration line are individual Reuters <u><b>articles</b></u> marked up with SGML tags, as <u><b>described</b></u> below. VI.A. The REUTERS <u><b>tag:</b></u> Each article <u><b>starts</b></u> with an &quot;open tag&quot; of the form &lt;REUTERS TOPICS=?? LEWISSPLIT=?? CGISPLIT=?? OLDID=?? NEWID=??&gt; where the <u><b>??</b></u> are filled in an appropriate fashion. Each article <u><b>ends</b></u> with a &quot;close tag&quot; of the <u><b>form:</b></u> &lt;/REUTERS&gt; In all <u><b>cases</b></u> the &lt;REUTERS&gt; and &lt;/REUTERS&gt; <u><b>tags</b></u> are the only <u><b>items</b></u> on their line. Each REUTERS tag <u><b>contains</b></u> explicit <u><b>specifications</b></u> of the <u><b>values</b></u> of five attributes, TOPICS, LEWISSPLIT, CGISPLIT, OLDID, and NEWID. These <u><b>attributes</b></u> are meant to identify <u><b>documents</b></u> and <u><b>groups</b></u> of documents, and have the following <u><b>meanings:</b></u> 1. TOPICS <u><b>:</b></u> The possible <u><b>values</b></u> are YES, NO, and BYPASS: a. YES <u><b>indicates</b></u> that *in the original data* there was at least one entry in the TOPICS fields. b. NO <u><b>indicates</b></u> that *in the original data* the story had no <u><b>entries</b></u> in the TOPICS field. c. BYPASS <u><b>indicates</b></u> that *in the original data* the story was marked with the string &quot;bypass&quot; (or a typographical variant on that string). This <u><b>poorly-named</b></u> attribute unfortunately is the subject of much confusion. It is meant to indicate whether or not the document had TOPICS <u><b>categories</b></u> *in the raw Reuters-22173 dataset*. The sole use of this attribute is to <u><b>defining</b></u> training set <u><b>splits</b></u> similar to those used in previous research. (See the section on training set splits.) The TOPICS attribute does **NOT** indicate anything about whether or not the Reuters-21578 document <u><b>has</b></u> any TOPICS categories. (Version 1.0 of this document was errorful on this point.) That can be determined by actually looking at the TOPICS field. A story with TOPICS=&quot;YES&quot; can have no TOPICS categories, and a story with TOPICS=&quot;NO&quot; can have TOPICS categories. Now, a reasonable (though not certain) assumption is that for all TOPICS=&quot;YES&quot; <u><b>stories</b></u> the indexer at least thought about whether the story <u><b>belonged</b></u> to a valid TOPICS category. Thus, the TOPICS=&quot;YES&quot; <u><b>stories</b></u> with no <u><b>topics</b></u> can reasonably be considered negative <u><b>examples</b></u> for all <u><b>135</b></u> valid TOPICS categories. TOPICS=&quot;NO&quot; <u><b>stories</b></u> are more problematic in their interpretation. Some of them presumedly result because the indexer made an explicit decision that they did not belong to any of the <u><b>135</b></u> valid TOPICS categories. However, there are many <u><b>cases</b></u> where it is clear that a story should belong to one or more TOPICS categories, but for some reason the category was not assigned. There appear to be certain time <u><b>intervals</b></u> where large <u><b>numbers</b></u> of such <u><b>stories</b></u> are concentrated, suggesting that some <u><b>parts</b></u> of the data set were simply not indexed, or not indexed for some <u><b>categories</b></u> or category sets. Also, in a few cases, the indexer clearly meant to assign TOPICS categories, but put them in the wrong field. These <u><b>cases</b></u> have been corrected in the Reuters-21578 data, yielding <u><b>stories</b></u> that have TOPICS categories, but where TOPICS=&quot;NO&quot;, because the the category was not assigned in the raw version of the data. &quot;BYPASS&quot; <u><b>stories</b></u> clearly were not indexed, and so are useful only for general distributional information on the language used in the documents. 2. LEWISSPLIT <u><b>:</b></u> The possible <u><b>values</b></u> are TRAINING, TEST, and NOT-USED. TRAINING <u><b>indicates</b></u> it was used in the training set in the <u><b>experiments</b></u> <u><b>reported</b></u> in LEWIS91d (Chapters <u><b>9</b></u> and 10), LEWIS92b, LEWIS92e, and LEWIS94b. TEST <u><b>indicates</b></u> it was used in the test set for those experiments, and NOT-USED <u><b>means</b></u> it was not used in those experiments. 3. CGISPLIT <u><b>:</b></u> The possible <u><b>values</b></u> are TRAINING-SET and PUBLISHED-TESTSET <u><b>indicating</b></u> whether the document was in the training set or the test set for the <u><b>experiments</b></u> <u><b>reported</b></u> in HAYES89 and HAYES90b. 4. OLDID <u><b>:</b></u> The identification number (ID) the story had in the Reuters-22173 collection. 5. NEWID <u><b>:</b></u> The identification number (ID) the story <u><b>has</b></u> in the Reuters-21578, Distribution 1.0 collection. These IDs are assigned to the <u><b>stories</b></u> in chronological order. In addition, some REUTERS <u><b>tags</b></u> have a sixth attribute, CSECS, which can be ignored. The use of these <u><b>attributes</b></u> is critical to <u><b>allowing</b></u> comparability between different <u><b>studies</b></u> with the collection, and is <u><b>discussed</b></u> further in Section VIII. VI.B. Document-Internal Tags Just as the &lt;REUTERS&gt; and &lt;/REUTERS&gt; <u><b>tags</b></u> serve to delimit <u><b>documents</b></u> within a file, other <u><b>tags</b></u> are used to delimit <u><b>elements</b></u> within a document. We discuss these in the order in which they typically appear, though the exact order should not be <u><b>relied</b></u> upon in processing. In some cases, additional <u><b>tags</b></u> occur within an element <u><b>delimited</b></u> by these top level <u><b>document-internal</b></u> tags. These are <u><b>discussed</b></u> in this section as well. We specify below whether each <u><b>open/close</b></u> tag pair is used exactly once (ONCE) per a story, or a variable (VARIABLE) number of times (possibly zero). In many <u><b>cases</b></u> the start tag of a pair <u><b>appears</b></u> only at the beginning of a line, with the corresponding end tag always <u><b>appearing</b></u> at the end of the same line. When this is the case, we indicate it with the notation &quot;SAMELINE&quot; below, as an aid to those <u><b>processing</b></u> the <u><b>files</b></u> without SGML tools. 1. &lt;DATE&gt;, &lt;/DATE&gt; [ONCE, SAMELINE]: Encloses the date and time of the document, possibly <u><b>followed</b></u> by some <u><b>non-date</b></u> noise material. 2. &lt;MKNOTE&gt;, &lt;/MKNOTE&gt; [VARIABLE] <u><b>:</b></u> Notes on certain hand <u><b>corrections</b></u> that were done to the original Reuters corpus by Steve Finch. 3. &lt;TOPICS&gt;, &lt;/TOPICS&gt; [ONCE, SAMELINE]: Encloses the list of TOPICS categories, if any, for the document. If TOPICS <u><b>categories</b></u> are present, each will be <u><b>delimited</b></u> by the <u><b>tags</b></u> &lt;D&gt; and &lt;/D&gt;. 4. &lt;PLACES&gt;, &lt;/PLACES&gt; [ONCE, SAMELINE]: Same as &lt;TOPICS&gt; but for PLACES categories. 5. &lt;PEOPLE&gt;, &lt;/PEOPLE&gt; [ONCE, SAMELINE]: Same as &lt;TOPICS&gt; but for PEOPLE categories. 6. &lt;ORGS&gt;, &lt;/ORGS&gt; [ONCE, SAMELINE]: Same as &lt;TOPICS&gt; but for ORGS categories. 7. &lt;EXCHANGES&gt;, &lt;/EXCHANGES&gt; [ONCE, SAMELINE]: Same as &lt;TOPICS&gt; but for EXCHANGES categories. 8. &lt;COMPANIES&gt;, &lt;/COMPANIES&gt; [ONCE, SAMELINE]: These <u><b>tags</b></u> always appear adjacent to each other, since there are no COMPANIES <u><b>categories</b></u> assigned in the collection. 9. &lt;UNKNOWN&gt;, &lt;/UNKNOWN&gt; [VARIABLE]: These <u><b>tags</b></u> bracket control <u><b>characters</b></u> and other noisy <u><b>and/or</b></u> somewhat mysterious material in the Reuters stories. 10. &lt;TEXT&gt;, &lt;/TEXT&gt; [ONCE]: We have <u><b>attempted</b></u> to delimit all the textual material of each story between a pair of these tags. Some control <u><b>characters</b></u> and other &quot;junk&quot; material may also be included. The <u><b>whitespace</b></u> structure of the text <u><b>has</b></u> been preserved. The &lt;TEXT&gt; tag <u><b>has</b></u> the following <u><b>attribute:</b></u> a. TYPE: This <u><b>has</b></u> one of three <u><b>values:</b></u> NORM, BRIEF, and UNPROC. NORM is the default value and <u><b>indicates</b></u> that the text of the story had a normal structure. In this case the TEXT tag <u><b>appears</b></u> simply as &lt;TEXT&gt;. The tag <u><b>appears</b></u> as &lt;TEXT TYPE=&quot;BRIEF&quot;&gt; when the story is a short one or two line note. The <u><b>tags</b></u> <u><b>appears</b></u> as &lt;TEXT TYPE=&quot;UNPROC&quot;&gt; when the format of the story is unusual in some fashion that limited our ability to further structure it. The following <u><b>tags</b></u> optionally delimit <u><b>elements</b></u> inside the TEXT element. Not all <u><b>stories</b></u> will have these <u><b>tags:</b></u> a. &lt;AUTHOR&gt;, &lt;/AUTHOR&gt; <u><b>:</b></u> Author of the story. b. &lt;DATELINE&gt;, &lt;/DATELINE&gt; <u><b>:</b></u> Location the story <u><b>originated</b></u> from, and day of the year. c. &lt;TITLE&gt;, &lt;/TITLE&gt; <u><b>:</b></u> Title of the story. We have <u><b>attempted</b></u> to capture the text of <u><b>stories</b></u> with TYPE=&quot;BRIEF&quot; within a &lt;TITLE&gt; element. d. &lt;BODY&gt;, &lt;/BODY&gt; <u><b>:</b></u> The main text of the story. VII. Categories A test collection for text categorization contains, at minimum, a set of <u><b>texts</b></u> and, for each text, a specification of what <u><b>categories</b></u> that text <u><b>belongs</b></u> to. For the Reuters-21578 collection the <u><b>documents</b></u> are Reuters <u><b>newswire</b></u> stories, and the <u><b>categories</b></u> are five different <u><b>sets</b></u> of content related categories. For each document, a human indexer decided which <u><b>categories</b></u> from which <u><b>sets</b></u> that document <u><b>belonged</b></u> to. The category <u><b>sets</b></u> are as <u><b>follows:</b></u> Number of Number of Categories Number of Categories Category Set Categories <u><b>w/</b></u> <u><b>1+</b></u> Occurrences <u><b>w/</b></u> <u><b>20+</b></u> Occurrences ************ ********** ******************** ******************** EXCHANGES <u><b>39</b></u> <u><b>32</b></u> <u><b>7</b></u> ORGS <u><b>56</b></u> <u><b>32</b></u> <u><b>9</b></u> PEOPLE <u><b>267</b></u> <u><b>114</b></u> <u><b>15</b></u> PLACES <u><b>175</b></u> <u><b>147</b></u> <u><b>60</b></u> TOPICS <u><b>135</b></u> <u><b>120</b></u> <u><b>57</b></u> The TOPICS <u><b>categories</b></u> are economic subject categories. Examples include &quot;coconut&quot;, &quot;gold&quot;, &quot;inventories&quot;, and &quot;money-supply&quot;. This set of <u><b>categories</b></u> is the one that <u><b>has</b></u> been used in almost all previous research with the Reuters data. HAYES90b <u><b>discusses</b></u> some <u><b>examples</b></u> of the <u><b>policies</b></u> (not always obvious) used by the human <u><b>indexers</b></u> in <u><b>deciding</b></u> whether a document <u><b>belonged</b></u> to a particular TOPIC category. The EXCHANGES, ORGS, PEOPLE, and PLACES <u><b>categories</b></u> correspond to <u><b>named</b></u> <u><b>entities</b></u> of the <u><b>specified</b></u> type. Examples include &quot;nasdaq&quot; (EXCHANGES), &quot;gatt&quot; (ORGS), &quot;perez-de-cuellar&quot; (PEOPLE), and &quot;australia&quot; (PLACES). Typically a document assigned to a category from one of these <u><b>sets</b></u> explicitly <u><b>includes</b></u> some form of the category name in the <u><b>document's</b></u> text. (Something which is usually not true for TOPICS categories.) However, not all <u><b>documents</b></u> <u><b>containing</b></u> a <u><b>named</b></u> entity corresponding to the category name are assigned to these category, since the entity was <u><b>required</b></u> to be a focus of the news story [HAYES90b]. Thus these proper name <u><b>categories</b></u> are not as simple to assign correctly as might be thought. Reuters-21578, Distribution 1.0 <u><b>includes</b></u> five <u><b>files</b></u> (all-exchanges-strings.lc.txt, all-orgs-strings.lc.txt, all-people-strings.lc.txt, all-places-strings.lc.txt, and all-topics-strings.lc.txt) which list the <u><b>names</b></u> of *all* legal <u><b>categories</b></u> in each set. A sixth file, cat-descriptions_120396.txt <u><b>gives</b></u> some additional information on the category sets. Note that a sixth category field, COMPANIES, was present in the original Reuters <u><b>materials</b></u> distributed by Carnegie Group, but no company information was actually included in these fields. In the Reuters-21578 collection this field is always empty. In the table above we note how many <u><b>categories</b></u> appear in at least <u><b>1</b></u> of the 21,578 <u><b>documents</b></u> in the collection, and how many appear at least <u><b>20</b></u> of the documents. Many <u><b>categories</b></u> appear in no documents, but we encourage <u><b>researchers</b></u> to include these <u><b>categories</b></u> when <u><b>evaluating</b></u> the effectiveness of their categorization system. Additional <u><b>details</b></u> of the documents, categories, and corpus preparation process appear in LEWIS92b, and at greater length in Section 8.1 of LEWIS91d. VIII. Using Reuters-21578 for Text Categorization Research In testing a method for text categorization it is important that knowledge of the nature of the test data not unduly influence the development of the system, or the performance <u><b>obtained</b></u> will be <u><b>unrealistically</b></u> high. One way of dealing with this is to divide a set of data into two <u><b>subsets:</b></u> a training set and a test set. An experimenter then <u><b>develops</b></u> a categorization system by <u><b>automated</b></u> training on the training set only, <u><b>and/or</b></u> by human knowledge engineering based on examination of the training set only. The categorization system is then tested on the previously unexamined test set. A number of <u><b>variations</b></u> on this basic theme are <u><b>possible---see</b></u> WEISS91 for a good discussion. Effectiveness <u><b>results</b></u> can only be <u><b>compared</b></u> between <u><b>studies</b></u> that the same training and test set (or that use <u><b>cross-validation</b></u> procedures). One problem with the Reuters-22173 collection was that the ambiguity of <u><b>formatting</b></u> and annotation led different <u><b>researchers</b></u> to use different <u><b>training/test</b></u> divisions. This was particularly problematic when <u><b>researchers</b></u> <u><b>attempted</b></u> to remove <u><b>documents</b></u> that &quot;had no TOPICS&quot;, as there were several <u><b>definitions</b></u> of what this meant. To eliminate these <u><b>ambiguities</b></u> from the Reuters-21578 collection we specify exactly which <u><b>articles</b></u> are in each of the <u><b>recommended</b></u> training <u><b>sets</b></u> and test <u><b>sets</b></u> by <u><b>specifying</b></u> the <u><b>values</b></u> those <u><b>articles</b></u> will have on the TOPICS, LEWISSPLIT, and CGISPLIT <u><b>attributes</b></u> of the REUTERS tags. We strongly encourage that all <u><b>studies</b></u> on Reuters-21578 use one of the following training test <u><b>divisions</b></u> (or use multiple random splits, e.g. cross-validation): VIII.A. The Modified Lewis (&quot;ModLewis&quot;) Split: Training Set (13,625 docs): LEWISSPLIT=&quot;TRAIN&quot;; TOPICS=&quot;YES&quot; or &quot;NO&quot; Test Set (6,188 docs): LEWISSPLIT=&quot;TEST&quot;; TOPICS=&quot;YES&quot; or &quot;NO&quot; Unused (1,765): LEWISSPLIT=&quot;NOT-USED&quot; or TOPICS=&quot;BYPASS&quot; This <u><b>replaces</b></u> the <u><b>14704/6746</b></u> split (723 unused) of the Reuters-22173 collection, which was used in LEWIS91d (Chapters <u><b>9</b></u> and 10), LEWIS92b, LEWIS92c, LEWIS92e, and LEWIS94b. Note the <u><b>following:</b></u> 1. The duplicate <u><b>documents</b></u> removed in forming Reuters-21578 are of course not present. 2. The <u><b>documents</b></u> with TOPICS=&quot;BYPASS&quot; are not used, since subsequent analysis strongly <u><b>indicates</b></u> that they were not <u><b>categorized</b></u> by the indexers. 3. The 1,765 unused <u><b>documents</b></u> should not be tested on and should not be used for <u><b>supervised</b></u> learning. However, they may useful as additional information on the statistical distribution of words, phrases, and other <u><b>features</b></u> that might used to predict categories. This split <u><b>assigns</b></u> <u><b>documents</b></u> from April 7, <u><b>1987</b></u> and before to the training set, and <u><b>documents</b></u> from April 8, <u><b>1987</b></u> and after to the test set. WARNING: Given the many <u><b>changes</b></u> in going from Reuters-22173 to Reuters-21578, <u><b>including</b></u> correction of many typographical <u><b>errors</b></u> in category labels, <u><b>results</b></u> on the ModLewis split cannot be <u><b>compared</b></u> with any <u><b>published</b></u> <u><b>results</b></u> on the Reuters-22173 <u><b>collection!</b></u> VIII.B. The Modified Apte (&quot;ModApte&quot;) Split <u><b>:</b></u> Training Set (9,603 docs): LEWISSPLIT=&quot;TRAIN&quot;; TOPICS=&quot;YES&quot; Test Set (3,299 docs): LEWISSPLIT=&quot;TEST&quot;; TOPICS=&quot;YES&quot; Unused (8,676 docs): LEWISSPLIT=&quot;NOT-USED&quot;; TOPICS=&quot;YES&quot; or TOPICS=&quot;NO&quot; or TOPICS=&quot;BYPASS&quot; This <u><b>replaces</b></u> the <u><b>10645/3672</b></u> split (7,856 not used) of the Reuters-22173 collection. These are our best approximation to the training and test <u><b>splits</b></u> used in APTE94 and APTE94b. Note the <u><b>following:</b></u> 1. As with the ModLewis, those <u><b>documents</b></u> removed in forming Reuters-21578 are not present, and BYPASS <u><b>documents</b></u> are not used. 2. The intent in APTE94 and APTE94b was to use the Lewis split, but restrict it to <u><b>documents</b></u> with at least one TOPICS categories. However, but it was not clear exactly what Apte, <u><b>et</b></u> al meant by <u><b>having</b></u> at least one TOPICS category (e.g. how was &quot;bypass&quot; treated, whether this was before or after any fixing of typographical errors, etc.). We have <u><b>encoded</b></u> our interpretation in the TOPICS attribute. ***Note that, as <u><b>discussed</b></u> above, some TOPICS=&quot;YES&quot; <u><b>stories</b></u> have no TOPICS categories, and a few TOPICS=&quot;NO&quot; <u><b>stories</b></u> have TOPICS categories. These <u><b>facts</b></u> are irrelevant to the definition of the split.*** If you are <u><b>using</b></u> a learning algorithm that <u><b>requires</b></u> each training document to have at least TOPICS category, you can screen out the training <u><b>documents</b></u> with no TOPICS categories. Please do NOT screen out any of the 3,299 <u><b>documents</b></u> <u><b>-</b></u> that will make your <u><b>results</b></u> incomparable with other studies. 3. As with ModLewis, it may be desirable to use the 8,676 Unused <u><b>documents</b></u> for gathering statistical information about feature distribution. As with ModLewis, this split <u><b>assigns</b></u> <u><b>documents</b></u> from April 7, <u><b>1987</b></u> and before to the training set, and <u><b>documents</b></u> from April 8, <u><b>1987</b></u> and after to the test set. The difference is that only <u><b>documents</b></u> with at least one TOPICS category are used. The rationale for this restriction is that while some <u><b>documents</b></u> lack TOPICS <u><b>categories</b></u> because no TOPICS apply (i.e. the document is a true negative example for all TOPICS categories), it <u><b>appears</b></u> that <u><b>others</b></u> simply were never assigned TOPICS <u><b>categories</b></u> by the indexers. (Unfortunately, the amount of time that <u><b>has</b></u> <u><b>passed</b></u> since the collection was <u><b>created</b></u> <u><b>has</b></u> made it difficult to establish exactly what went on during the indexing.) WARNING: Given the many <u><b>changes</b></u> in going from Reuters-22173 to Reuters-21578, <u><b>including</b></u> correction of many typographical <u><b>errors</b></u> in category labels, <u><b>results</b></u> on the ModApte split cannot be <u><b>compared</b></u> with any <u><b>published</b></u> <u><b>results</b></u> on the Reuters-22173 <u><b>collection!</b></u> VIII.C. The Modified Hayes (&quot;ModHayes&quot;) Split: Training Set (20856 docs): CGISPLIT=&quot;TRAINING-SET&quot; Test Set (722 docs): CGISPLIT=&quot;PUBLISHED-TESTSET&quot; Unused (0 docs) This is the best approximation we have to the training and test <u><b>splits</b></u> used in HAYES89, HAYES90b, and Chapter <u><b>8</b></u> of LEWIS91d. It <u><b>replaces</b></u> the <u><b>21450/723</b></u> split of the Reuters-22173 collection. Note the <u><b>following:</b></u> 1. As with the other splits, the duplicate <u><b>documents</b></u> removed in forming Reuters-21578 are not present. 2. &quot;Training&quot; in HAYES89 and HAYES90b was actually done by human <u><b>beings</b></u> looking at the <u><b>documents</b></u> and writing categorization rules. We can not be sure which of the document <u><b>files</b></u> were actually <u><b>looked</b></u> at. 3. We specify that the BYPASS <u><b>stories</b></u> and the TOPICS=NO <u><b>stories</b></u> are part of the training set, since they were used during manual knowledge engineering in the original Hayes experiments. That does not mean <u><b>researchers</b></u> are obliged to give these <u><b>stories</b></u> to, for instance, a <u><b>supervised</b></u> learning algorithm. As <u><b>mentioned</b></u> in the other splits, they may be more useful for getting distributional information about features. There are a number of <u><b>problems</b></u> with the ModHayes split that make it less than desirable for text categorization research, <u><b>including</b></u> unusual distribution of categories, <u><b>pairs</b></u> of <u><b>near-duplicate</b></u> documents, and chronological burstiness. (See [LEWIS90b, Ch. <u><b>8]</b></u> for more details.) Despite these problems, this split is of interest because it <u><b>provides</b></u> the ability to compare <u><b>results</b></u> with those of the CONSTRUE system [HAYES89, HAYES90b]. Comparison of <u><b>results</b></u> on the ModHayes split with previously <u><b>published</b></u> <u><b>results</b></u> on the original Hayes split in HAYES89 and HAYES90b (and LEWIS90b, Ch. 8) is possible, though the following <u><b>points</b></u> should be taken into <u><b>account:</b></u> 1. The <u><b>testset</b></u> we provide in the ModHayes split <u><b>has</b></u> one <u><b>fewer</b></u> document than the one Hayes used. The document that was removed (OLDID=&quot;22026&quot;) was a <u><b>timestamp</b></u> duplicate of the document with OLDID=&quot;22027&quot; and NEWID=&quot;13234&quot;. So in <u><b>computing</b></u> effectiveness <u><b>measures</b></u> for comparison with HAYES89/90b, the document with NEWID=&quot;13234&quot; should be <u><b>counted</b></u> twice. 2. The <u><b>documents</b></u> in the Hayes <u><b>testset</b></u> had relatively few <u><b>errors</b></u> and <u><b>anomalies</b></u> in their categorization. And the <u><b>errors</b></u> which we did find and correct appear unlikely to have affected the original Hayes results. In particular, it <u><b>appears</b></u> that the only <u><b>errors</b></u> in the TOPICS field were the addition of a few invalid <u><b>categories</b></u> that were not <u><b>evaluated</b></u> on. However, for completeness we list the <u><b>changes</b></u> in the Hayes <u><b>testset</b></u> <u><b>documents</b></u> made going from Reuters-22173 to Reuters-21578 (all <u><b>documents</b></u> are <u><b>referred</b></u> to by their NEWID): Removal of invalid TOPIC &quot;loan&quot; <u><b>:</b></u> 13234, 16946, 17111, 17112, 17207, 17217, 17228, 17234, 17271, <u><b>17310</b></u> Removal of invalid TOPIC &quot;gbond&quot; <u><b>:</b></u> 17138, <u><b>17260</b></u> Removal of invalid TOPIC &quot;tbill&quot; <u><b>:</b></u> <u><b>17258</b></u> Removal of invalid TOPIC &quot;cbond&quot; <u><b>:</b></u> <u><b>17024</b></u> Removal of invalid TOPIC &quot;fbond&quot; <u><b>:</b></u> <u><b>17087</b></u> Correction of invalid PEOPLE <u><b>mancera</b></u> to <u><b>mancera-aguayo:</b></u> 17142, 17149, 17154, 17177, <u><b>17187</b></u> Correction of invalid PEOPLE <u><b>andriesssen</b></u> to <u><b>andriessen</b></u> <u><b>:</b></u> <u><b>17366</b></u> Correction of invalid PLACES &quot;ivory&quot; and &quot;coast&quot; to single correct PLACE &quot;ivory-coast&quot;: <u><b>18383</b></u> 3. The effectiveness <u><b>measures</b></u> used in HAYES89 and HAYES90b were somewhat nonstandard. See Ch. <u><b>8</b></u> of LEWIS91d for a discussion. VIII.D. Other Splits We strongly encourage <u><b>researchers</b></u> to use one (or more) of the above <u><b>splits</b></u> for their <u><b>experiments</b></u> (or use <u><b>cross-validation</b></u> on one of the <u><b>sets</b></u> of <u><b>documents</b></u> defined in the above splits). We recommend the Modified Apte (&quot;ModApte&quot;) Split for research on <u><b>predicting</b></u> the TOPICS field, since the evidence is that a significant number of <u><b>documents</b></u> that should have TOPICS do not. The ModLewis split can be used if the researcher <u><b>has</b></u> a strong need to test the ability of a system to deal with <u><b>examples</b></u> belonging to no category. While it is likely that some of these <u><b>examples</b></u> should indeed belong to a category, the ModLewis split is at least better than the corresponding split from Reuters-22173, in that it <u><b>eliminates</b></u> the &quot;bypass&quot; stories. We in particular encourage you to resist the following <u><b>temptations:</b></u> 1. Defining new <u><b>splits</b></u> based on whether or not the <u><b>documents</b></u> actually have any TOPICS categories. (See the discussion of the ModApte split.) 2. Testing your system only on the &quot;easy&quot; categories. This is a temptation we have <u><b>succumbed</b></u> to in the past, but will resist in the future. Yes, we know that some of the <u><b>135</b></u> TOPICS <u><b>categories</b></u> have few or no positive training <u><b>examples</b></u> or few or no positive test <u><b>examples</b></u> or both. Yes, purely <u><b>supervised</b></u> learning <u><b>systems</b></u> will do very badly on these categories. Knowledge-based systems, on the other hand, might do well on them, while doing poorly in comparison with <u><b>supervised</b></u> learning on <u><b>categories</b></u> with lots of positive examples. These <u><b>comparisons</b></u> are of great interest. Of course, <u><b>it's</b></u> of great interest to *in addition* analyze <u><b>subsets</b></u> of <u><b>categories</b></u> (e.g. lots of positive <u><b>examples</b></u> vs. few positive examples, etc.). Note that one strategy we considered and <u><b>rejected</b></u> is to assume that <u><b>documents</b></u> which have no TOPICS but do have <u><b>categories</b></u> in other <u><b>fields</b></u> (PLACES, etc.) could be assumed to belong to no TOPICS categories. This does not appear to be a safe assumption <u><b>-</b></u> we have found a number of <u><b>examples</b></u> of <u><b>documents</b></u> with PLACES but no TOPICS when there are TOPICS that clearly apply. IX. Feature Sets in Text Categorization For many text categorization methods, particularly those <u><b>using</b></u> statistical classification techniques, it is convenient to represent <u><b>documents</b></u> not as a sequence of characters, but rather as a <u><b>tuple</b></u> of <u><b>numeric</b></u> or binary feature values. For instance, the value of feature Fi for a document Dj might be <u><b>1</b></u> if the string of <u><b>characters</b></u> &quot;financial&quot; <u><b>occurred</b></u> in the document with <u><b>whitespace</b></u> on either side, and <u><b>0</b></u> otherwise. Or the value of Fi for Dj might be the number of <u><b>occurrences</b></u> of &quot;financial&quot; in document Dj. In information retrieval such <u><b>features</b></u> are often <u><b>called</b></u> &quot;indexing terms&quot; and one often <u><b>speaks</b></u> of a term being &quot;present&quot; in a document, to mean that the feature <u><b>takes</b></u> on a <u><b>non-default</b></u> value. (Usually, but not always, any value but <u><b>0</b></u> is non-default.) Comparisons between text categorization <u><b>methods</b></u> that represent <u><b>documents</b></u> as feature <u><b>tuples</b></u> are <u><b>aided</b></u> by <u><b>ensuring</b></u> that the same <u><b>tuple</b></u> representation is used with all methods, thus <u><b>avoiding</b></u> <u><b>conflating</b></u> <u><b>differences</b></u> in feature extraction with <u><b>differences</b></u> in, say, machine learning methods. For that reason, the Reuters-22173 distribution included not only the <u><b>formatted</b></u> text of the Reuters stories, but also feature <u><b>tuple</b></u> <u><b>representations</b></u> of the <u><b>stories</b></u> in each of two feature sets, one based on <u><b>words</b></u> and one based on noun phrases. Surprisingly, almost no use was made of these <u><b>files</b></u> by other researchers, so we have not included <u><b>files</b></u> of this sort in the Reuters-21578 distribution. However, we are willing to make available as part of the distribution any <u><b>tuple</b></u> <u><b>representations</b></u> of this sort that <u><b>researchers</b></u> want to contribute. (Contact lewis@research.att.com if you would like to do this.) Perhaps the ideal situation would be if someone with a strong interest in feature set formation produced <u><b>tuples</b></u> based on a high quality set of <u><b>features</b></u> which other <u><b>researchers</b></u> interested only in learning <u><b>algorithms</b></u> could make use of. 